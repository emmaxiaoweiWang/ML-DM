First lesson in Tensorflow

I guess it is beacuse the data unit in this tool is tensor and the algorithm is all about connecting these data together they name it 
tensor flow. The rank of a tensor can be more than 2. And the rank is number of dimensions of the tensor. Tensor is just an array.

Today I install it on windows using python's pip3 tool and try some scripts. A constant is also a tensor. You can create a node of a constant.
Until you create a session object and invoke the run method of session object, you can get the real value the value of the node created before.

Computational graph is composed of different nodes. Each node accepts tensors as input and output tensors. Constant is also a kind of node.

There are differences between placeholder and variables. Variabes can be initialized while placeholder is not. And variables are not initialized 
until global_variables_initializer is invoked.

So what's the whole point of machine learning in you opinion?
Well, the whole point of machine leanring is about finding the correct parameters.

Getting started with Tensorflow:
https://www.tensorflow.org/get_started/get_started

Just like we have 'hello word program' in programming, machine learning has MNIST data set which is a data set of labeled images. These
images are labeled from 0 to 9. It is natural to use softmax regression to assign probabilites to an object being one of several different things.
Because softmax regression gives us a list of values that add up to 1. 
So let's begin to put softmax regression into action by using tensorflow.

SOFTMAX REGRESSION IN TENSOR FLOW
Softmax regression first exponentiate the linear function between weights and piexl intensity and bias, and then normalize it.
Why does SR use exponentiate? Because it assumes that one pixel showing evidence that it in one class should be a multiplied with previous 
evidence. When a negative evidence should be less of unit and also reduce the confidence generated by prevous evidences.

YUAN WEN RU XIA :
"But it's often more helpful to think of softmax the first way: exponentiating its inputs and then normalizing them. 
The exponentiation means that one more unit of evidence increases the weight given to any hypothesis multiplicatively. 
And conversely, having one less unit of evidence means that a hypothesis gets a fraction of its earlier weight. 
No hypothesis ever has zero or negative weight. Softmax then normalizes these weights, so that they add up to one, 
forming a valid probability distribution. (To get more intuition about the softmax function, check out the section 
on it in Michael Nielsen's book, complete with an interactive visualization.)"







